---
title: "Alex's Notes and Text Fragments for the JOSS Manuscript"
author: "Alexander Brenning"
date: "21 Apr 2021"
output: html_document
bibliography: paper.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Proposed New Introduction

Spatial and spatio-temporal prediction tasks are common in applications ranging from environmental sciences to archaeology and epidemiology. While sophisticated mathematical frameworks have long been developed in spatial statistics to characterize predictive uncertainties under well-defined mathematical assumptions such as intrinsic stationarity [e.g., cressie.2015], computational estimation procedures have only been proposed more recently to assess predictive performances of spatial and spatio-temporal prediction models [@brenning.2005.nhess; @sperrorest; @pohjankukka2017; @roberts2017].

Although alternatives such as the bootstrap exist and have some advantages [@efron.gong.1983, or David Hand], cross-validation is a particularly well-established, easy-to-implement algorithm for *model assessment* of supervised machine-learning models [@efron.gong.1983 and next section] and *model selection* [@arlot.celisse.2010]. In its basic form, it is based on resampling the data without paying attention to any possible dependence structure, which may arise from, e.g., cluster sampling [ref], grouped data [@brenning.lausen.2008; @pena.brenning.2015.maipo], or underlying environmental processes inducing some sort of spatial coherence at the landscape scale [@brenning.2005.nhess]. In treating dependent observations as independent, or ignoring autocorrelation, cross-validation test samples may in fact be heavily correlated with, or even pseudo-replicates of, the data used for training the model, which introduces a potentially severe bias, especially for flexible machine-learning models [@brenning.2005.nhess].

This cross-validation bias is well-known in spatial as well as non-spatial prediction [@brenning.2005.nhess; @brenning.lausen.2008; @arlot.celisse.2010]. It is most easily understood from a predictive modelling perspective by focusing on the question where (and when) the model should be used for prediction. In crop classification from remotely-sensed data, for instance, learning samples routinely contain multiple grid cells from a sample of fields with known crop type, e.g. 2000 grid cells from 100 fields scattered across a large study region. The purpose of training a model on this sample is to make predictions on other, new *fields* within the same geographic domain [*intra-domain* prediction; @brenning.2005.nhess] -- not *within* the same field, which obviously presents only one crop type. It would therefore be foolish to train a model on a simple random subsample of grid cells, and test it on the remaining data, i.e. using other grid cells from the same fields, as if we wanted to predict within a field; the results from this performance assessment would be over-optimitic, and perhaps badly so. To mimic the predictive situation for which the model is trained, we'd rather have to resample at the level of fields, not grid cells [@pena.brenning.2015.maipo]. If the model was to be applied to adjacent agricultural districts, i.e. outside the learning sample's spatial domain [*extra-domain* prediction; @brenning.2005.nhess], it would even seem necessary to resample at a higher level of spatial aggregation, i.e. at the level of agricultural districts within the learning sample, in order to realistically mimic the actual prediction task. The cross-validation resampling needed therefore depends as much on the prediction task as on the data structure or dependency at hand.

While it is not the purpose of this paper to recommend specific resampling schemes for specific use cases, this example may suffice to motivate the use of spatial and spatio-temporal cross-validation techniques, and the need for a unified framework and computational toolbox that accommodate a variety of prediction tasks that may be applicable to a broad range of prediction tasks. `mlr3spatiotemporalcv` is such a toolbox.

This toolbox, implemented as an open-source R package, builds upon and generalizes several existing toolboxes that have been developed in recent years for more specific settings. The earliest of these implementations is the `sperrorest` package [@sperrorest], which has, however, not been integrated into established machine-learning frameworks such as `mlr` or `caret`, and lacks support of temporal prediction tasks. Other more recent implementations include `blockCV` for (what for?) [ref @blockCV; @rest2014] and `CAST` for spatio-temporal prediction tasks in a `caret` framework (correct?) [@cast; @meyer2018a]. And `skmeans` [ref]. Reference to Table 1 FIXME. 

Thus, `mlr3spatiotemporalcv` implements for the first time a comprehensive state-of-the-art compilation of spatial and spatio-temporal partitioning schemes that is well-integrated into a major machine-learning framework in R, the `mlr3` ecosystem [@mlr3]. This package is furthermore equipped with a variety of two- and three-dimensional diagram types. Our hope is that implementation will facilitate reproducible geospatial modelling and code-sharing across a broad range of application domains.



## Proposed section 2: Spatial and Spatio-temporal Cross-validation

the formal stuff, next on my list...

In cross-validation for predictive model assessment, we consider the following formal setting. We are interested in predicting a numerical or categorical response $y$ of an object or instance using a feature vector $\mathbf{x} = (x^{(1)}, \ldots, x^{(p)}\in\mathbb{R}$ and a model $\hat{f}_L$ that has been trained on a learning sample $L = \{(y_i, \mathbf{x}_i),\ i = 1, ..., n\}$. Our goal is to estimate the expected value of the performance of $\hat{f}_L$,
\[
\mathit{perf(\hat{f}_L)} := E(l(Y,\hat{f}_L(X))),
\]
where $l$ is a real-valued loss function, and the expected value is with respect the probability distribution of $X$, the features of an instance $(Y,X)$ drawn randomly from the underlying population. This is referred to as the *actual* or *conditional* performance measure as it is conditional on $L$ [@hand.1996]. The loss function can take a variety of forms such as the misclassification error $I(Y\neq\hat{f}_L(X))$ in classification, or the squared error $(Y-\hat{f}_L(X))^2$ in regression, among many other possible measures. The choice of the performance measure is equally critical as the choice of the estimation procedure, but it is beyond the scope of this contribution to discuss performance metrics for regression and classification [see e.g., @hand.1996 for classification, and ref?? for regression tasks].

Nevertheless, since we only have a sample $T$ of test data drawn from the population, we can realistically only *estimate* the conditional performance of $\hat{f}_L$:
\[
\widehat{\mathit{perf}}_T(\hat{f}_L) = \frac{1}{|T|}\sum_{i=1}^{|T|}l(Y,\hat{f}_L(X)).
\]
This representation as a point estimator for $\mathit{perf(\hat{f}_L)}$ underlines the importance of using a random sample for performance estimation to avoid estimation bias. Other estimators than the simple mean may be required when $T$ is not a simple random sample, for instance a stratified random sample [e.g., @thompson.2012]. As always, judgment sampling may lead to uncontrollable bias.

Since the re-use of the learning sample $L$ for testing, $T=L$, would result in the over-optimistic *resubstitution* or *apparent* model performance, cross-validation partitions the sample $L$ into disjoint training and test sets. Specifically, $L$ is split into $k$ partitions,
\[
L = L_1 \cup \ldots \cup L_k,\qquad L_i\cap L_j = \emptyset\quad \textrm{for all}\ i\neq j,
\]
and a model $\hat{f}_{(i)}$ is fitted on $L_{(i)} := L\setminus L_i$, while $L_i$ is withheld for testing. This is repeated for $i=1,\ldots,k$ in order to effectively use the entire sample for testing, while keeping training and test sets disjoint at all times. The $k$-fold CV estimator can therefore be written as
\[
\widehat{\mathit{perf}}_{L, CV}(f) := \frac{1}{k}\sum_{i=1}^k\widehat{\mathit{perf}}_{L_i}(\hat{f}_{L_{(i)}}),
\]
where $f$ is a machine-learning algorithm, i.e. a mapping that trains a model $\hat{f}_S$ using any suitable training sample $S$. The use of $k=5$ or $k=10$ folds is most commonly seen in practice, and these preferences are also supported by theory [ref?? where did I get this from?]. The $k$-fold CV estimator of model performance is a nearly unbiased estimator of the conditional performance measure when the observations were drawn independently [@efron.gong.1983]. Since $\widehat{\mathit{perf}}_{L, CV}(f)$ still depends on the particular partitioning chosen for $L$, it is sometimes recommended to repeat the estimation using different random partitionings ($r$-repeated $k$-fold cross-validation, e.g. $k=100$; ref??).

In traditional cross-validation, the partitioning is based on uniform random sampling, which ignores spatial or temporal autocorrelation or any existing grouping structure as well as the structure of the prediction task, as outlined in the Introduction, and may result in over-optimistic performance estimates. Several approaches have therefore been proposed in the literature and implemented in software to accommodate a variety of predictive situations (ref Table 1 FIXME).

etc

A variant of cross-validation is leave-one-out (LOO) cross-validation , which has long been established in geostatistics [@cressie.2015], sometimes with a focus on the spatial distribution of LOO error [@willmott.matsuura.2006]. Since this is just a special case of non-spatial CV with $k=n$, it is sometimes also referred to as spatial CV [@willmott.matsuura.2006]. Nevertheless, spatial variants have been proposed that apply an exclusion buffer to the test locations to separate them from the training data [@brenning.2005.nhess]. One plausible approach for this is to use the range of autocorrelation of model residuals to determine the buffer distance as this ensures independence conditional on the predictors [@brenning.2005.nhess]. When data is grouped, e.g., due to multi-level sampling designs or the study of spatial objects, LOO at has been proposed at the site level [@martin.et.al.2008; @kasurak.et.al.2011] or, in animal movement studies, at the animal level [@anderson.et.al.2005]. Similarly, in the temporal domain, it has been proposed to leave out temporal observational units, as in leave-one-year-out CV [@anderson.et.al.2005; @brenning.2005.nhess].

It should be noted that $k$-fold cross-validation with a large value of $k$, and LOO-CV in particular, are not only very time-consuming since the model has to be trained $k$ times; these models will also be nearly identical since only a tiny fraction of the data is withheld, and therefore estimation bias increases. 'Pure' LOO-CV is therefore not recommended for machine-learning model assessment [ref??].


## General Comments

- Avoid the concept of SAC, TAC and STAC because it lacks a clear definition (in this paper, and a definition is not at all straightforward), and the link between STAC and a specific type of CV is far from obvious. Instead, refer to spatial, temporal, and spatio-temporal prediction tasks.

- I.e. the question is not "what's the structure of my data", but "what's the structure of the prediction problem". Perhaps this recommendation can be included in the Discussion.


## References

