---
# Example from https://joss.readthedocs.io/en/latest/submitting.html
title: "mlr3spatiotempcv: Spatiotemporal Resampling Methods for 'mlr3'"
tags:
  - resampling
  - machine learning
  - autocorrelation
  - spatial
  - temporal
authors:
  - name: Patrick Schratz
    orcid: 0000-0003-0748-6624
    affiliation: "1, 2"
  - name: Marc Becker
    orcid: 0000-0002-8115-0400
    affiliation: 2
  - name: Michel Lang
    orcid: 0000-0001-9754-0393
    affiliation: "2, 3"
affiliations:
 - name: Friedrich-Schiller-University Jena
   index: 1
 - name: Ludwig-Maximilians-University Munich
   index: 2
 - name: TU Dortmund University
   index: 3
citation_author: Schratz et. al
date: 01 November 2020
year: 2020
bibliography: paper.bib
# documentclass: joss
output: rticles::joss_article
csl: apa.csl
# journal: JOSS
---

# Summary

The R package [mlr3spatiotempcv](https://mlr3spatiotempcv.mlr-org.com) is an extension package of the machine learning framework [mlr3](https://mlr3.mlr-org.com) [@mlr3].
It adds support for spatiotemporal resampling methods and respective visualization methods.
Spatiotemporal resampling methods are needed to account for the existence of different autocorrelation types (spatial autocorrelation (SAC), temporal autocorrelation (TAC) or spatio-temporal autocorrelation (STAC)) in datasets.
These autocorrelation types cause biased performance estimates of varying magnitude in cross-validation when ignored.

At the time of writing, various R packages implementing different spatiotemporal partitioning strategies exist: CAST [@cast], blockCV [@blockCV] or sperrorest [@brenning2012].
There are some resampling methods used in scientific studies for which the source code is either not available or a different programming language has been used [@roberts2017, @pohjankukka2017]

The goal of [mlr3spatiotempcv](https://mlr3spatiotempcv.mlr-org.com) is to gather the available spatiotemporal resampling methods in R and make them available to users through a simplified and generic interface.
The latter is made possible by integrating the package directly into the [mlr3](https://mlr3.mlr-org.com) machine learning framework.
This simplifies the step of integrating package specific syntax into an overarching machine learning pipeline.
The hope is that the availability of such a package encourages researchers to more often apply spatiotemporal resampling methods in scientific case studies.

### Spatiotemporal Autocorrelation in Machine Learning

Data which includes spatial or temporal information requires special treatment in machine learning; similar to cost-sensitive, functional, multilabel, ordinal or survival datasets [@mlr3book].
In contrast to non-spatial/non-temporal data, observations inherit a natural grouping, either in space or time or in both space and time [@legendre1993].
This grouping causes observations to be autocorrelated, either in space (spatial autocorrelation (SAC)), time (temporal autocorrelation (TAC)) or both space and time (spatiotemporal autocorrelation (STAC)).
For simplicity, the acronym STAC is used as a generic term in the following discussion for all the different characteristics introduced above.

*What effects does STAC have in statistical/machine learning?*

The overarching problem is that STAC violates the assumption that the observations in the train and test datasets are independent [@hastie2001].
If this assumption is violated, the reliability of the resulting performance estimates, for example retrieved via cross-validation, is decreased.
The magnitude of this decrease is linked to the magnitude of STAC in the dataset, which cannot be determined easily.

One approach to account for the existence of STAC is to use dedicated resampling methods.
[mlr3spatiotempcv](https://mlr3spatiotempcv.mlr-org.com) provides access to the most frequently used spatiotemporal resampling methods.
The following example, which uses the [ecuador](https://mlr3spatiotempcv.mlr-org.com/reference/mlr_tasks_ecuador.html) dataset created by [Jannes Muenchow](https://scholar.google.com/citations?user=Slq94Y4AAAAJ&hl=de&authuser=1&oi=ao), showcases how a spatial dataset can be used to retrieve a bias-reduced performance estimate of a learner.

This dataset contains information on the occurrence of landslides (binary) in the Andes of Southern Ecuador.
The landslides were mapped from aerial photos taken in 2000.
The dataset is well suited to serve as an example because it is relatively small and, of course, due to the spatial nature of the observations.
Please refer to @muenchow2012 for a detailed description of the dataset.

To account for the spatial autocorrelation probably present in the landslide data, we will make use of one of the most used spatial partitioning methods, a cluster-based $k$-means grouping [@brenning2012], (`"spcv_coords"` in [mlr3spatiotempcv](https://mlr3spatiotempcv.mlr-org.com)).
This method performs a clustering in 2D space which contrasts with the commonly used random partitioning for non-spatial data.
The grouping has the effect that train and test data are more separated in space than they would be by conducting a random partitioning, thereby reducing the effect of STAC.

By contrast, when using the classical random partitioning approach with spatial data, train and test observations would be located side-by-side across the full study area (a visual example is provided further below).
This leads to a high similarity between train and test sets, resulting in "better" but over-optimistic performance estimates in every fold of a CV compared to the Spatial CV (SpCV) approach.
However, these low error rates are mainly caused due to the STAC in the observations and the lack of appropriate partitioning methods and not by the power of the fitted model.

Rephrased in other words: the higher performance results in a non-spatial CV scenario should not be relied on or used for reporting.
The positive bias in these results is substantial and the actual performance of the model is nowhere near this result.
Results from a SpCV are not unbiased but are closer in reflecting the actual performance of the learner on unknown datasets.

## Comparing Spatial Resampling & Non-Spatial Resampling

In the following, a spatial and a non-spatial CV will be conducted to showcase the mentioned performance differences.

The performance of a simple classification tree (`"classif.ranger"`) is evaluated on a random repeated cross-validation (`"repeated_cv"`) with four folds and two repetitions.
The chosen evaluation measure is the "Area Under the ROC Curve" (`"classif.auc"`).
The only difference in the spatial setting is that `"repeated_spcv_coords"` is chosen instead of `"repeated_cv"`.

### Data Preparation

The following code chunk first loads all necessary R packages and sets a low verbosity for this example.
A seed is set for deterministic execution.

Next the mlr3 builtin task `"ecuador"` is loaded via `tsk()`, which is a convenience wrapper around the underlying R6 construction method for a spatiotemporal classification task (`mlr3spatiotempcv::TaskClassifST$new()`).

Last, the random forest learner (`"classif.ranger")` is initialized with default hyperparameters and the prediction type set to `"probability"` due to the binary response variable.

```{r setup, echo=FALSE}
# important for correct figure placement
# see https://stackoverflow.com/a/49395389/4185785
knitr::knit_hooks$set(plot = function(x, options) {
  knitr::hook_plot_tex(x, options)
})
```

```{r prepare}
library("mlr3")
library("mlr3spatiotempcv")
library("mlr3learners")
set.seed(42)

# be less verbose
lgr::get_logger("bbotk")$set_threshold("warn")
lgr::get_logger("mlr3")$set_threshold("warn")

task = tsk("ecuador")

learner = lrn("classif.ranger", predict_type = "prob")
```

### Non-Spatial Cross-Validation

```{r rr-nspcv}
resampling_nsp = rsmp("repeated_cv", folds = 4, repeats = 2)
rr_nsp = resample(
  task = task, learner = learner,
  resampling = resampling_nsp
)

rr_nsp$aggregate(measures = msr("classif.auc"))
```

### Spatial Cross-Validation

```{r rr-spcv}
resampling_sp = rsmp("repeated_spcv_coords", folds = 4, repeats = 2)
rr_sp = resample(
  task = task, learner = learner,
  resampling = resampling_sp
)

rr_sp$aggregate(measures = msr("classif.auc"))
```

Here, the estimated AUC using Non-Spatial Cross-Validation (NSpCV) is around 0.15 better compared to the Spatial Cross-Validation.
Remember that the better performance score results from over-optimism in the estimation with NSpCV.
The actual predictive performance on new data is much more realistically reflected by the performance estimated via SpCV.
The magnitude of this difference is variable as it depends on the dataset, the magnitude of STAC and the learner itself.
Algorithms with a higher tendency of overfitting to the training set will have a large spread in such scenarios.

## Visualization of Spatiotemporal Partitions

Every partitioning method in [mlr3spatiotempcv](https://mlr3spatiotempcv.mlr-org.com) comes with a generic `plot()` method to visualize the created groups.
In a 2D space this happens via [ggplot2](https://ggplot2.tidyverse.org) [@ggplot2] while for spatiotemporal methods 3D visualizations via [plotly](https://github.com/ropensci/plotly) [@plotly] are created.

```{r vis-spcv, fig.cap="Visualization of spatial partitioning", fig.pos="!b", fig.height=6, fig.width=8}
autoplot(resampling_sp, task, fold_id = c(1:4)) *
  ggplot2::scale_y_continuous(breaks = seq(-3.97, -4, -0.01)) *
  ggplot2::scale_x_continuous(breaks = seq(-79.06, -79.08, -0.01))
```
\

Unless specified by the user, the coordinate reference system (CRS) defaults to EPSG code 4326 (WGS84).
This is because a lat/lon based CRS is better suited for plotting purposes than a Mercator (UTM) one.
Note that setting the correct CRS for the given data *during construction* is very important.
Even though EPSG 4326 is a good fallback and often used for visualization purposes, spatial offsets of up to multiple meters may occur if the wrong CRS was passed initially.

This example used an already created task via the convenience function `tsk()`.
In practice however, one needs to create a spatiotemporal task via `TaskClassifST()`/`TaskRegrST()` and set the `crs` argument.

The spatial grouping of the k-means based approach above (Figure 1) contrasts visually very well compared to the NSpCV (random) partitioning (Figure 2):

```{r vis-nspcv, fig.cap="Visualization of random partitioning", fig.pos="!b", fig.height=6, fig.width=8}
autoplot(resampling_nsp, task, fold_id = c(1:4), crs = 4326) *
  ggplot2::scale_y_continuous(breaks = seq(-3.97, -4, -0.01)) *
  ggplot2::scale_x_continuous(breaks = seq(-79.06, -79.08, -0.01))
```

## Choosing a Resampling Method

While the example used the `"spcv_coords"` method, this does not mean that this method is the best or only method suitable for this task.
Even though this method is quite popular, it was mainly chosen because of the clear visual grouping differences compared to random partitioning.

In fact, most often multiple spatial partitioning methods can be used for a dataset.
It is recommended (required) that users familiarize themselves with each implemented method and decide which method to choose based on the specific characteristics of the dataset.
For almost all methods implemented in [mlr3spatiotempcv](https://mlr3spatiotempcv.mlr-org.com), there is a scientific publication describing the strengths and weaknesses of the respective approach (either linked in the help file of [mlr3spatiotempcv](https://mlr3spatiotempcv.mlr-org.com) or its respective dependency packages).

In the example above, a cross-validation without hyperparameter tuning was shown.
If a nested CV is desired, it is recommended to use the same spatial partitioning method for the inner loop (= tuning level).
See @schratz2019 for more details and chapter 11 of [Geocomputation with R](https://geocompr.robinlovelace.net/spatial-cv.html) [@lovelace2019].

A list of all implemented methods in [mlr3spatiotempcv](https://mlr3spatiotempcv.mlr-org.com) can be found in the [Getting Started](https://mlr3spatiotempcv.mlr-org.com/articles/mlr3spatiotempcv.html) vignette of the package.

# References
